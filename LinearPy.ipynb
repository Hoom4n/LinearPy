{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b226511-e28b-4eb7-919e-9d50be1bff95",
   "metadata": {},
   "source": [
    "<center><h2>LinearPy</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d104bef1-2297-401f-b216-4d48f19186d6",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "\n",
    "[1. Introduction](#1.-Introduction)\n",
    "  - [1.1. Introduction](#1.1.-Introduction)\n",
    "  - [1.2. Utility Functions](#1.2.-Utility-Functions)\n",
    "\n",
    "[2. Linear Regression](#2.-Linear-Regression)\n",
    "  - [2.1. Linear Modeling](#2.1.-Linear-Modeling)\n",
    "  - [2.2. Closed Form Solution: Normal Equation](#2.2.-Closed-Form-Solution:-Normal-Equation)\n",
    "  - [2.3. Batch Gradient Descent (with Regularization)](#2.3.-Batch-Gradient-Descent-(with-Regularization))\n",
    "  - [2.4. Mini Batch Gradient Descent (with Regularization & Learning Schedule)](#2.4.-Mini-Batch-Gradient-Descent-(with-Regularization-&-Learning-Schedule))\n",
    "  - [2.5. Stochastic Gradient Descent (with Regularization & Learning Schedule)](#2.5.-Stochastic-Gradient-Descent-(with-Regularization-&-Learning-Schedule))\n",
    "\n",
    "[3. Linear Classification](#3.-Linear-Classification)\n",
    "  - [3.1. Logistic Regression (with Regularization & Learning Schedule)](#3.1.-Logistic-Regression-(with-Regularization-&-Learning-Schedule))\n",
    "  - [3.2. Softmax Regression (with Regularization & Learning Schedule)](#3.2.-Softmax-Regression-(with-Regularization-&-Learning-Schedule))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bff488f-9dd0-4eec-91fc-63bf84670c13",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869b3f4a-7915-42e8-ad05-94ced8a7fe5d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1.1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82d21b68-7cbf-4cf0-87c3-23a543cbeda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4287a9e9-46dd-4495-8b7f-0810b6793f5a",
   "metadata": {},
   "source": [
    "This notebook presents a structured walkthrough of **Linear Modeling Techniques**, moving from theoretical foundations and mathematical formulations to practical implementations—each model is built as a custom predictor class designed to align with the style of **Scikit-Learn** (though the library itself is not used).\n",
    "\n",
    "\n",
    "By implementing and inspecting these models from scratch, we gain a deeper intuition for how supervised learning tasks work under the hood. Linear models form one of the most important foundations in machine learning, powering both traditional algorithms and modern deep learning architectures.\n",
    "\n",
    "We begin with simple **Linear Regression**, then introducing **Gradient Descent**, an **Iterative Optimization technique**  and its variants (batch, mini-batch, and stochastic). From there, we move to **Linear Classification**, covering both **Logistic Regression** (for binary classification) and **Softmax Regression** (for multi-class problems).\n",
    "\n",
    "To enhance learning and model performance, we also explore techniques like **Regularization** and **Learning Rate Schedules**, which are critical for controlling overfitting and improving optimization.\n",
    "\n",
    "---\n",
    "**Project Requirements**\n",
    "\n",
    "The only dependency for this project is **NumPy**, a powerful library for numerical computing and linear algebra. All implementations—from data preprocessing to model evaluation—are built entirely using NumPy, with no external ML libraries.\n",
    "\n",
    "---\n",
    "**Project Structure**\n",
    "\n",
    "Since Scikit-Learn is not used, I have implemented several utility functions to support model evaluation and experimentation:\n",
    "\n",
    "* **Synthetic Data Generation**\n",
    "\n",
    "  * Functions to generate polynomial, non-linear, and multi-class datasets for both regression and classification tasks.\n",
    "   \n",
    "\n",
    "* **Preprocessing Utilities**\n",
    "\n",
    "  * `StandardScaler` for feature normalization.\n",
    "  * `OneHotEncoder` for categorical targets in classification.\n",
    "  * `train_test_split` for test/val partitioning.\n",
    "\n",
    "* **Evaluation Metrics**\n",
    "\n",
    "  * Regression: `MSE`, `RMSE`, `MAE`\n",
    "  * Classification: `Accuracy`, `Precision`, `Recall`, `F1 Score`\n",
    "  * A general `ModelEvaluator` class to summarize results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a86b5b9-4b90-42d0-b4af-824632003ec4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1.2. Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060cd684-c2ff-45c0-ae05-566600a3b660",
   "metadata": {},
   "source": [
    "**Dataset Generation**\n",
    "\n",
    "To generate data for evaluating models, I used a simple polynomial formula with added noise to simulate real-world conditions:\n",
    "\n",
    "- **Regression**  \n",
    "  $x_{\\text{poly}} = x^2$  \n",
    "  $y = w \\cdot x_{\\text{poly}} + \\varepsilon$\n",
    "\n",
    "- **Classification**  \n",
    "  $x_{\\text{poly}} = x^2$  \n",
    "  $\\text{logits} = W \\cdot x_{\\text{poly}} + \\varepsilon$  \n",
    "      <i>If binary:</i> $y = \\text{sigmoid}(\\text{logits})$  \n",
    "      <i>If multiclass:</i> $y = \\text{softmax}(\\text{logits})$\n",
    "\n",
    "*Where:*\n",
    "\n",
    "$w, W$ = randomly initialized weights  \n",
    "$\\varepsilon$ = random noise (Gaussian)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab532bf5-9500-435a-96b6-a05fd091ad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate a polynomial degree-2 regression dataset\n",
    "\n",
    "def generate_poly_regression_data(n_samples=700, n_features=10, noise_std=1.0, random_state=42):\n",
    "    \"\"\" generates a polynomial degree-2 regression dataset \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    X = 6 * np.random.rand(n_samples, n_features)\n",
    "    \n",
    "    X_poly = np.c_[X, X**2] # degree-2 poly\n",
    "    \n",
    "    coef = np.random.randn(X_poly.shape[1])\n",
    "    \n",
    "    y = X_poly @ coef + np.random.normal(0, noise_std, size=n_samples) # final equation\n",
    "    \n",
    "    return X_poly, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc49d80f-7bfb-4a3a-9bd4-3bf13d1d95a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate a polynomial degree-2 classification dataset\n",
    "\n",
    "def generate_poly_classification_data(n_samples=700, n_features=10, n_classes=3, noise_std=1.0, random_state=42):\n",
    "    \"\"\" generates a polynomial degree-2 classification dataset \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    X = 6 * np.random.rand(n_samples, n_features)\n",
    "     \n",
    "    X_poly = np.c_[X, X**2] # degree-2 poly\n",
    "    \n",
    "    coef = np.random.randn(X_poly.shape[1], n_classes)\n",
    "    \n",
    "    logits = X_poly @ coef + np.random.normal(0, noise_std, size=(n_samples, n_classes))\n",
    "    \n",
    "    if n_classes == 2:\n",
    "        probs = 1 / (1 + np.exp(-logits[:, 0]))  # Sigmoid\n",
    "        y = (probs >= 0.5).astype(int)\n",
    "    else:\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)  # Softmax\n",
    "        y = np.argmax(probs, axis=1)\n",
    "    return X_poly, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9457d46-922f-4eb8-b53b-e730ce51e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler:\n",
    "    \"\"\" normalizes matrix X by standard scaling\"\"\"\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "\n",
    "    def fit(self,X):\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        self.std = np.std(X, axis=0)\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        return (X - self.mean) / self.std  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b926ab5-1012-4ea7-b0e6-c48e1da5f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias(X):\n",
    "    \"\"\" adds a bias/y-intercept to matrix X\"\"\"\n",
    "    return np.c_[np.ones(X.shape[0]) , X] #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "393a525f-b421-4fba-b783-bfbd0b4bf195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHotEncoding(X):\n",
    "    ohe = np.zeros((X.size,X.max() + 1))\n",
    "    ohe[np.arange(X.size) , X] = 1\n",
    "    return ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc2cf740-e251-4ba1-b7b9-4660d05ae89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Performance Metrics\n",
    "\n",
    "def mean_squared_error(y_true,y_pred):\n",
    "    return np.mean((y_pred.reshape(-1,1) - y_true.reshape(-1,1))**2)\n",
    "\n",
    "def root_mean_squared_error(y_true,y_pred):\n",
    "    return np.sqrt(np.mean((y_pred.reshape(-1,1) - y_true.reshape(-1,1))**2))\n",
    "\n",
    "def mean_absolute_error(y_true,y_pred):\n",
    "    return np.mean(np.absolute(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "\n",
    "def r2_score(y_true,y_pred):\n",
    "    return 1 - ( np.sum((y_pred.reshape(-1,1) - y_true.reshape(-1,1))**2) / np.sum((y_true.reshape(-1,1) - np.mean(y_true.reshape(-1,1)))**2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abd22f07-81ea-449c-bacd-3a1b3f0c49d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classidication Performance Metrics (for both binary and mulit-class classification)\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    y_true, y_pred = np.ravel(y_true), np.ravel(y_pred)\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def precision(y_true, y_pred, average='binary'):\n",
    "    y_true, y_pred = np.ravel(y_true), np.ravel(y_pred)\n",
    "    labels = np.unique(np.concatenate((y_true, y_pred)))\n",
    "    precisions = []\n",
    "    for label in labels:\n",
    "        tp = np.sum((y_pred == label) & (y_true == label))\n",
    "        fp = np.sum((y_pred == label) & (y_true != label))\n",
    "        precisions.append(tp / (tp + fp) if (tp + fp) > 0 else 0.0)\n",
    "    if average == 'binary':\n",
    "        return precisions[1] if len(precisions) > 1 else precisions[0]\n",
    "    return np.mean(precisions)\n",
    "\n",
    "def recall(y_true, y_pred, average='binary'):\n",
    "    y_true, y_pred = np.ravel(y_true), np.ravel(y_pred)\n",
    "    labels = np.unique(np.concatenate((y_true, y_pred)))\n",
    "    recalls = []\n",
    "    for label in labels:\n",
    "        tp = np.sum((y_pred == label) & (y_true == label))\n",
    "        fn = np.sum((y_pred != label) & (y_true == label))\n",
    "        recalls.append(tp / (tp + fn) if (tp + fn) > 0 else 0.0)\n",
    "    if average == 'binary':\n",
    "        return recalls[1] if len(recalls) > 1 else recalls[0]\n",
    "    return np.mean(recalls)\n",
    "\n",
    "def f1_score(y_true, y_pred, average='binary'):\n",
    "    p = precision(y_true, y_pred, average)\n",
    "    r = recall(y_true, y_pred, average)\n",
    "    return 2 * p * r / (p + r) if (p + r) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "535c89de-5e7f-410d-abd5-027e47fa531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sampling a val/test set\n",
    "\n",
    "def train_test_split(X,y,random_state=42, train_size=0.8):\n",
    "    np.random.seed(random_state)\n",
    "    rnd_indx = np.random.permutation(len(X)) # Shuffling indexes\n",
    "    \n",
    "    train_end_size = int(len(X) * train_size)\n",
    "    \n",
    "    X_train,y_train = X[rnd_indx[:train_end_size]] , y[rnd_indx[:train_end_size]] \n",
    "    X_test,y_test = X[rnd_indx[train_end_size:]] , y[rnd_indx[train_end_size:]] \n",
    "    \n",
    "    return X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17dac1e7-9e9a-4b63-8946-487351f673ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_regression(model, X_train, y_train, X_val, y_val):\n",
    "    \"\"\" returns train/val regression metrics \"\"\"\n",
    "    print(\"-\" * 35)\n",
    "    print(f\"Val Target True Mean: {y_val.mean():.4f} | Std Dev: {y_val.std():.4f}\")\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    print(f\"RMSE: {root_mean_squared_error(y_val, y_pred):.4f}\")\n",
    "    print(f\"MAE: {mean_absolute_error(y_val, y_pred):.4f}\")\n",
    "    print(f\"R² Score: {r2_score(y_val, y_pred):.4f}\")\n",
    "    print(\"-\" * 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21357fe2-6358-42b2-ba26-81224cef5f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(model, X_train, y_train, X_val, y_val, average=\"macro\"):\n",
    "    \"\"\" returns train/val classificaion metrics. supports macro-average scores for multi-class \"\"\"\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    print(f\"Accuracy:  {accuracy_score(y_val, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision(y_val, y_pred, average):.4f}\")\n",
    "    print(f\"Recall:    {recall(y_val, y_pred, average):.4f}\")\n",
    "    print(f\"F1 Score:  {f1_score(y_val, y_pred, average):.4f}\")\n",
    "    \n",
    "    print(\"-\" * 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166be869-c680-472d-adca-dbb8a5558990",
   "metadata": {},
   "source": [
    "## 2. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09859cf6-c144-40aa-a6f2-90a1d52dbde8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.1. Linear Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1d5386-3836-42b6-94fb-ca6ff20b2181",
   "metadata": {},
   "source": [
    "**Introduction to Linear Modeling**\n",
    "\n",
    "Linear modeling is one of the most basic and widely used methods in supervised machine learning. It aims to capture the relationship between input features and a target variable by assuming this relationship can be approximated as a weighted sum of the features.\n",
    "\n",
    "Given input features $\\mathbf{X} = [x_1, x_2, \\dots, x_n]$ and parameters (weights) $\\boldsymbol{\\theta} = [\\theta_0, \\theta_1, \\dots, \\theta_n]^T$, the model predicts an output $\\hat{y}$ as:  \n",
    "\n",
    "$$\n",
    "\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n\n",
    "$$  \n",
    "\n",
    "Here, $\\theta_0$ is the bias term (or intercept), which allows the model to shift predictions up or down regardless of the input features. The other parameters $\\theta_1, \\dots, \\theta_n$ represent how much each feature contributes to the prediction. \n",
    "\n",
    "The \"linear\" aspect refers to the fact that the prediction is a linear combination of these parameters. Importantly, the features themselves don’t have to be simple — they can be transformed or expanded into nonlinear functions like squares or sine waves, as long as the model remains linear in terms of $\\boldsymbol{\\theta}$. This flexibility lets linear models handle a surprising range of problems while keeping the math and computation manageable.\n",
    "\n",
    "Example: Imagine you want to estimate house prices based on features like the size of the house ($x_1$) and the number of bedrooms ($x_2$). A linear model expresses the predicted price $\\hat{y}$ as:  \n",
    "\n",
    "$$\n",
    "\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2\n",
    "$$  \n",
    "\n",
    "The weights $\\theta_1$ and $\\theta_2$ quantify how much each feature influences the price, while $\\theta_0$ adjusts the baseline price independent of those features.\n",
    "\n",
    "---\n",
    "\n",
    "Formulation of Linear Regression\n",
    "\n",
    "Linear regression is a key example of linear modeling where the goal is to find the best-fitting parameters $\\boldsymbol{\\theta}$ that minimize the difference between the model’s predictions and the actual target values. This difference is often measured by the mean squared error (MSE), which penalizes larger errors more heavily.\n",
    "\n",
    "For a dataset with $m$ samples and $n$ features, we organize the input into a design matrix $\\mathbf{X}$, which includes a column of ones to account for the bias term. The prediction for all samples at once can be written as:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{y}} = \\mathbf{X} \\boldsymbol{\\theta}\n",
    "$$\n",
    "\n",
    "The task of training the model is then to find the $\\boldsymbol{\\theta}$ that minimizes the overall error between these predictions $\\mathbf{\\hat{y}}$ and the true target values $\\mathbf{y}$.\n",
    "\n",
    "This approach forms the foundation for many machine learning models and is often the first step before moving on to more complex methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18c4c1c7-f71d-4d46-9039-0aeac2ba1498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prepartion\n",
    "\n",
    "# Generating data\n",
    "X , y = generate_poly_regression_data(n_samples=3000, n_features=10, noise_std=1.0, random_state=42)\n",
    "\n",
    "# train/val split\n",
    "X_train , y_train  , X_val , y_val = train_test_split( X , y , random_state=42, train_size=0.8)\n",
    "\n",
    "# scaling will help faster convergence\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992ff3de-0291-466b-beb3-9f03052ec22d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2. Closed Form Solution: Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c505757c-b364-429e-8531-0f7ad3dca814",
   "metadata": {},
   "source": [
    "**Solving Linear Regression via Singular Value Decomposition (SVD)**\n",
    "\n",
    "Given a dataset with $m$ samples and $n$ features, linear regression aims to find a parameter vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^{n+1}$ (including the bias term $\\theta_0$) that best fits the data. This means minimizing the mean squared error (MSE) between the predicted values $\\mathbf{\\hat{y}} = \\mathbf{X}\\boldsymbol{\\theta}$ and the true targets $\\mathbf{y} \\in \\mathbb{R}^m$. Here, the design matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times (n+1)}$ is the original features with an added column of ones to include the intercept. (What `add_bias` function does)\n",
    "\n",
    "The problem can be expressed as:\n",
    "\n",
    "$$\n",
    "\\min_{\\boldsymbol{\\theta}} \\sum_{i=1}^m \\left( (\\mathbf{X}\\boldsymbol{\\theta})_i - y_i \\right)^2\n",
    "$$\n",
    "\n",
    "\n",
    "When the matrix $\\mathbf{X}$ is full rank (meaning $\\mathbf{X}^T \\mathbf{X}$ is invertible), we can find the exact solution using the normal equation:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "However, this approach breaks down if $\\mathbf{X}$ is rank-deficient (for example, if features are highly correlated or if there are fewer samples than features), because then $\\mathbf{X}^T \\mathbf{X}$ becomes singular and cannot be inverted.\n",
    "\n",
    "To handle such cases more reliably, we use the Moore-Penrose pseudoinverse computed through Singular Value Decomposition (SVD). The SVD factorizes $\\mathbf{X}$ into three matrices:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $\\mathbf{U} \\in \\mathbb{R}^{m \\times m}$ and $\\mathbf{V} \\in \\mathbb{R}^{(n+1) \\times (n+1)}$ are orthogonal matrices,  \n",
    "- $\\mathbf{\\Sigma} \\in \\mathbb{R}^{m \\times (n+1)}$ is a diagonal matrix containing singular values $\\sigma_i \\geq 0$.  \n",
    "\n",
    "The pseudoinverse $\\mathbf{X}^+$ is then computed as:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^+ = \\mathbf{V} \\mathbf{\\Sigma}^+ \\mathbf{U}^T\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\Sigma}^+$ is formed by taking the reciprocal of each non-zero singular value $\\sigma_i$ and transposing the matrix. Using this pseudoinverse, the optimal parameters are:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta} = \\mathbf{X}^+ \\mathbf{y}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Numpy’s linear algebra module provides an implementation of the pseudoinverse via `np.linalg.pinv()`. Below is a straightforward implementation using SVD to solve linear regression with this approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70516ef3-862d-4a2a-a75e-c38d8a4c4575",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionSVD:\n",
    "    \"\"\"\n",
    "    Linear Regression model solved using Singular Value Decomposition (SVD).\n",
    "    This implementation uses the Moore-Penrose pseudoinverse to find the optimal parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self): \n",
    "        self.theta = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        y = y.reshape(-1, 1)\n",
    "        X = add_bias(X)  # Add the bias term to the design matrix\n",
    "        self.theta = np.linalg.pinv(X) @ y  # Calculate theta using the Moore-Penrose pseudoinverse of X\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = add_bias(X)\n",
    "        yhat = X @ self.theta  # Perform prediction using the learned parameters\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "428c385b-6a3f-4413-ad79-bd2292d2cf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Val Target True Mean: 57.8266 | Std Dev: 31.1542\n",
      "RMSE: 0.9922\n",
      "MAE: 0.7956\n",
      "R² Score: 0.9990\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Testing & Evaluating the model class\n",
    "\n",
    "linreg_svd = LinearRegressionSVD()\n",
    "\n",
    "evaluate_regression(linreg_svd, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03814b09-de83-4c13-97d2-d5913429dc2d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.3. Batch Gradient Descent (with Regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2a28ac-d164-4119-95c2-4968f2f16959",
   "metadata": {},
   "source": [
    "**Gradient Descent: An Iterative Approach to Linear Regression**\n",
    "\n",
    "Another way to find the parameters of a linear regression model is **gradient descent (GD)**, an iterative optimization method. Instead of directly solving for \\$\\theta\\$, GD minimizes the loss function step-by-step.\n",
    "\n",
    "The objective is to minimize the mean squared error (MSE):\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}_i - y_i)^2 = \\frac{1}{m} \\|X \\theta - y\\|^2\n",
    "$$\n",
    "\n",
    "To minimize \\$J(\\theta)\\$, we compute its derivative (gradient) with respect to \\$\\theta\\$:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J = \\frac{2}{m} X^T (X \\theta - y)\n",
    "$$\n",
    "\n",
    "Gradient descent updates parameters by moving opposite to the gradient:\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\eta \\nabla_\\theta J\n",
    "$$\n",
    "\n",
    "where \\$\\eta\\$ is the learning rate controlling the update size.\n",
    "\n",
    "---\n",
    "\n",
    "**Regularization: Controlling Model Complexity**\n",
    "\n",
    "To prevent overfitting and improve generalization, we add a penalty term to the loss, called **regularization**. This discourages large or complex parameter values.\n",
    "\n",
    "Two common types of regularization are:\n",
    "\n",
    "* **L1 regularization (Lasso)**: adds the sum of absolute values \\$|\\theta|\\$, encouraging sparsity.\n",
    "* **L2 regularization (Ridge)**: adds the sum of squared values \\$\\theta^2\\$, encouraging small weights.\n",
    "\n",
    "We can combine both in **Elastic Net** regularization:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\|X \\theta - y\\|^2 + \\lambda \\left( \\alpha |\\theta| + (1-\\alpha) \\theta^2 \\right)\n",
    "$$\n",
    "\n",
    "where \\$\\lambda\\$ controls how strong the penalty is, and \\$\\alpha\\$ balances between L1 and L2.\n",
    "\n",
    "Taking the derivative of this full loss gives the gradient:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J = \\frac{2}{m} X^T (X \\theta - y) + \\lambda \\left( \\alpha \\cdot \\text{sign}(\\theta) + 2(1-\\alpha) \\theta \\right)\n",
    "$$\n",
    "\n",
    "Gradient descent uses this gradient to iteratively update \\$\\theta\\$, shrinking weights to avoid overfitting. This is the key formula behind the update step in the `BatchGradientDescent` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4d743fe-f87a-49b3-827f-11d14ada8073",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGradientDescent:\n",
    "    \"\"\"\n",
    "    Batch Gradient Descent with Elastic Net Regularization for linear regression.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    eta : float, default=0.01\n",
    "        Learning rate (step size) for gradient descent.\n",
    "        Must be > 0. Too large may cause divergence, too small slows convergence.\n",
    "        \n",
    "    n_epochs : int, default=1000\n",
    "        Number of training iterations over the entire dataset.\n",
    "        Must be >= 1. Higher values may lead to better convergence but longer training.\n",
    "        \n",
    "    alpha : float, default=0\n",
    "        Mixing parameter for Elastic Net regularization:\n",
    "        - alpha=0: Pure L2 (Ridge) regularization\n",
    "        - alpha=1: Pure L1 (Lasso) regularization\n",
    "        - 0 < alpha < 1: Elastic Net mix\n",
    "        Must be between 0 and 1 (inclusive).\n",
    "        \n",
    "    lambda_ : float, default=0.01\n",
    "        Regularization strength.\n",
    "        Must be >= 0. Higher values increase regularization effect.\n",
    "        \n",
    "    random_state : int, default=42\n",
    "        Seed for random weight initialization.\n",
    "        Provides reproducibility of results.\n",
    "        \n",
    "    Attributes:\n",
    "    -----------\n",
    "    theta : ndarray of shape (n_features + 1, 1)\n",
    "        Learned weight vector (includes bias term).\n",
    "    \"\"\"\n",
    "    def __init__(self,eta=0.01,n_epochs=1000,alpha=0,lambda_=0.01, random_state=42):\n",
    "        self.eta = eta\n",
    "        self.n_epochs = n_epochs\n",
    "        self.alpha = alpha\n",
    "        self.lambda_ = lambda_\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        X = add_bias(X)\n",
    "        y = y.reshape(-1,1)\n",
    "        \n",
    "        np.random.seed(self.random_state)\n",
    "        self.theta = np.random.rand(X.shape[1],1)\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "\n",
    "            l1_grad = self.lambda_ * self.alpha * np.sign(self.theta)\n",
    "            l2_grad = self.lambda_ * (1-self.alpha) * 2 * self.theta\n",
    "            \n",
    "            gradients = (2 / X.shape[0]) * X.T @ (X @ self.theta - y) + l1_grad + l2_grad # gradient vector for MSE + ElasticNet Regularizer\n",
    "            self.theta = self.theta - self.eta * gradients\n",
    "        return self\n",
    "\n",
    "    def predict(self,X):\n",
    "        X = add_bias(X)\n",
    "        yhat = X @ self.theta\n",
    "        return yhat.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d981d01-c482-49b9-aae0-ceeff5faa903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Val Target True Mean: 57.8266 | Std Dev: 31.1542\n",
      "RMSE: 1.6950\n",
      "MAE: 1.3607\n",
      "R² Score: 0.9970\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Testing & Evaluating the model class\n",
    "\n",
    "batchGD = BatchGradientDescent(eta=0.01,n_epochs=4000,alpha=0,lambda_=0.01, random_state=42)\n",
    "\n",
    "evaluate_regression(batchGD, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6937fe4-641a-4aaa-b8ec-e1d2fe037162",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.4. Mini Batch Gradient Descent (with Regularization & Learning Schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db27d253-4c84-4b29-b09d-19e7057c0fb9",
   "metadata": {},
   "source": [
    "Mini-batch Gradient Descent (GD) is a variant of Gradient Descent where, in each iteration, parameters are updated based on a small subset (batch) of the training dataset, rather than the entire set.\n",
    "\n",
    "This approach is used for several reasons, including **out-of-core learning**: when the full dataset is too large to fit into memory, mini-batch GD allows processing only a portion of the data at a time, calculating parameter updates based solely on those samples.\n",
    "\n",
    "Unlike standard GD, where the cost function typically decreases smoothly toward the minimum, mini-batch GD causes the cost function to fluctuate, decreasing only on average. Over time, it approaches the minimum but continues to oscillate around it, never fully settling. As a result, the final parameter values are generally good but not optimal.\n",
    "\n",
    "**Learning Schedule**\n",
    "\n",
    "To address this issue, one approach is to reduce the learning rate to a very small value, which stabilizes updates near the optimal point and improves convergence. However, this can significantly slow down training, requiring many steps and epochs. A more effective solution is to implement a **learning schedule**, where the learning rate starts high to make rapid progress and gradually decreases over iterations. This balances faster convergence with achieving a solution close to the optimal point.\n",
    "\n",
    "In the class below, mini-batch Gradient Descent with L1 and L2 regularization and a simple learning schedule function has been implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a11d0012-1939-40a1-ac7f-7536dc10d51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatchGradientDescent:\n",
    "    \"\"\"\n",
    "    Mini-batch Gradient Descent with Elastic Net Regularization for linear regression.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    eta : float, default=0.01\n",
    "        Learning rate. Must be > 0. Large values risk divergence, small values slow convergence.\n",
    "    \n",
    "    n_epochs : int, default=1000\n",
    "        Number of epochs. Must be >= 1. Higher values improve convergence but increase training time.\n",
    "    \n",
    "    batch_size : int, default=64\n",
    "        Mini-batch size. Must be >= 1 and <= dataset size. Smaller batches add noise, larger ones increase computation.\n",
    "    \n",
    "    learning_schedule : bool, default=True\n",
    "        If True, decreases learning rate over iterations.\n",
    "    \n",
    "    learning_schedule_t0 : float, default=5\n",
    "        Controls initial learning rate decay. Must be > 0.\n",
    "    \n",
    "    learning_schedule_t1 : float, default=50\n",
    "        Controls decay rate. Must be > 0.\n",
    "    \n",
    "    alpha : float, default=0\n",
    "        Elastic Net mixing: 0 (L2/Ridge), 1 (L1/Lasso), (0,1) mix. Must be in [0,1].\n",
    "    \n",
    "    lambda_ : float, default=0.01\n",
    "        Regularization strength. Must be >= 0. Higher values increase regularization.\n",
    "    \n",
    "    random_state : int, default=42\n",
    "        Seed for reproducibility.\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.01, n_epochs=1000, batch_size=64, learning_schedule=True, \n",
    "                 learning_schedule_t0=5, learning_schedule_t1=50, alpha=0, lambda_=0.01, random_state=42):\n",
    "        self.eta = eta\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_schedule = learning_schedule\n",
    "        self.learning_schedule_t0 = learning_schedule_t0\n",
    "        self.learning_schedule_t1 = learning_schedule_t1\n",
    "        self.alpha = alpha\n",
    "        self.lambda_ = lambda_\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = add_bias(X)\n",
    "        y = y.reshape(-1, 1)\n",
    "        \n",
    "        np.random.seed(self.random_state)\n",
    "        self.theta = np.random.rand(X.shape[1], 1)\n",
    "\n",
    "        def learning_schedule(t):\n",
    "            return self.learning_schedule_t0 / (t + self.learning_schedule_t1)\n",
    "\n",
    "        update_count = 0\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            rnd_idx = np.random.permutation(len(X))\n",
    "            \n",
    "            for start in range(0, len(X), self.batch_size):\n",
    "                X_batch = X[rnd_idx[start:start + self.batch_size]]\n",
    "                y_batch = y[rnd_idx[start:start + self.batch_size]]\n",
    "\n",
    "                l1_grad = self.lambda_ * self.alpha * np.sign(self.theta)\n",
    "                l2_grad = self.lambda_ * (1 - self.alpha) * 2 * self.theta\n",
    "                \n",
    "                gradients = (2 / X_batch.shape[0]) * X_batch.T @ (X_batch @ self.theta - y_batch) + l1_grad + l2_grad \n",
    "                \n",
    "                if self.learning_schedule:\n",
    "                    self.eta = learning_schedule(update_count)\n",
    "                    \n",
    "                self.theta -= self.eta * gradients\n",
    "                update_count += 1\n",
    "                \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = add_bias(X)\n",
    "        return (X @ self.theta).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9cf726f4-56dc-46b1-91fc-a99b00e0c066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Val Target True Mean: 57.8266 | Std Dev: 31.1542\n",
      "RMSE: 1.6950\n",
      "MAE: 1.3607\n",
      "R² Score: 0.9970\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Testing & Evaluating the model class\n",
    "\n",
    "minibatchGD = MiniBatchGradientDescent(eta=0.01,n_epochs=4000,batch_size=64,learning_schedule=True, learning_schedule_t0=5,\n",
    "                                   learning_schedule_t1=50,alpha=0,lambda_=0.01, random_state=42)\n",
    "\n",
    "evaluate_regression(batchGD, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fa4ed8-0f6e-4104-983e-464de3774b77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.5. Stochastic Gradient Descent (with Regularization & Learning Schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8ea50d-fe28-46d0-86eb-793d69556153",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent (SGD) is a variant of Gradient Descent , same as MiniBatchGD, particularly useful when the dataset is too large to fit into memory. By processing only one sample per iteration, SGD is faster than Batch Gradient Descent. However, similar to Mini-batch GD but more pronounced, its path to the minimum is not smooth, exhibiting significant fluctuations and potentially overshooting the minimum due to its high variance. To mitigate this, a learning schedule is implemented to dynamically adjust the learning rate, starting high for rapid progress and decreasing over iterations for better convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c67eae77-fa5d-4b41-b790-c49b26c9a524",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticGradientDescent:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent with Elastic Net Regularization for linear regression.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    eta : float, default=0.01\n",
    "        Learning rate. Must be > 0. Large values risk divergence, small values slow convergence.\n",
    "    \n",
    "    n_epochs : int, default=500\n",
    "        Number of epochs. Must be >= 1. Higher values improve convergence but increase training time.\n",
    "    \n",
    "    learning_schedule : bool, default=True\n",
    "        If True, decreases learning rate over iterations.\n",
    "    \n",
    "    learning_schedule_t0 : float, default=5\n",
    "        Controls initial learning rate decay. Must be > 0.\n",
    "    \n",
    "    learning_schedule_t1 : float, default=50\n",
    "        Controls decay rate. Must be > 0.\n",
    "    \n",
    "    alpha : float, default=0\n",
    "        Elastic Net mixing: 0 (L2/Ridge), 1 (L1/Lasso), (0,1) mix. Must be in [0,1].\n",
    "\n",
    "    lambda_ : float, default=0.01\n",
    "        Regularization strength. Must be >= 0. Higher values increase regularization.\n",
    "    \n",
    "    random_state : int, default=42\n",
    "        Seed for reproducibility.\n",
    "    \"\"\"\n",
    "    def __init__(self,eta=0.01,n_epochs=500,learning_schedule=True, learning_schedule_t0=5,learning_schedule_t1=50,alpha=0,\n",
    "                 lambda_=0.01, random_state=42):\n",
    "        \n",
    "        self.eta = eta\n",
    "        self.n_epochs = n_epochs\n",
    "        self.learning_schedule = learning_schedule\n",
    "        self.learning_schedule_t0 = learning_schedule_t0\n",
    "        self.learning_schedule_t1 = learning_schedule_t1\n",
    "        self.alpha = alpha\n",
    "        self.lambda_ = lambda_\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        X = add_bias(X)\n",
    "        y = y.reshape(-1,1) # reshape to 1D array\n",
    "\n",
    "        np.random.seed(self.random_state)\n",
    "        self.theta = np.random.rand(X.shape[1],1) # an (n,) 1D param vector\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        def learning_schedule(t):\n",
    "            return self.learning_schedule_t0 / (t + self.learning_schedule_t1)\n",
    "            \n",
    "        for epoch in range(self.n_epochs):\n",
    "            rnd_idx = np.random.permutation(len(X)) # Shuffling indexes\n",
    "            \n",
    "            for iteration in range(m):\n",
    "                xi = X[rnd_idx[iteration]].reshape(-1,1)\n",
    "                yi = y[rnd_idx[iteration]]\n",
    "\n",
    "                if self.learning_schedule:\n",
    "                    self.eta = learning_schedule(epoch * m + iteration)\n",
    "\n",
    "                l1_grad = self.lambda_ * self.alpha * np.sign(self.theta)\n",
    "                l2_grad = self.lambda_ * (1-self.alpha) * 2 * self.theta\n",
    "                \n",
    "                gradients = 2 * xi * (xi.T @ self.theta - yi) + l1_grad + l2_grad\n",
    "                self.theta = self.theta - self.eta * gradients\n",
    "                \n",
    "        return self\n",
    "\n",
    "    def predict(self,X):\n",
    "        X = add_bias(X)\n",
    "        self.theta.reshape(-1,1)\n",
    "        yhat = X @ self.theta\n",
    "        return yhat.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ae5c58ce-ba4f-41f7-9997-984d9ab71810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Val Target True Mean: 57.8266 | Std Dev: 31.1542\n",
      "RMSE: 20.7468\n",
      "MAE: 16.9764\n",
      "R² Score: 0.5565\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Testing & Evaluating the model class\n",
    "\n",
    "stochasticGD = StochasticGradientDescent(eta=0.01,n_epochs=4000,learning_schedule=True, learning_schedule_t0=5,learning_schedule_t1=50,\n",
    "                                         alpha=0,lambda_=0.01, random_state=42)\n",
    "\n",
    "evaluate_regression(stochasticGD, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed4a0f8-c702-4907-9eb5-e813524a18e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b73d18-de5d-44d3-af40-c1ff51a8f421",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.1. Logistic Regression (with Regularization & Learning Schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21a34c0-9b24-414e-9282-35217d17250a",
   "metadata": {},
   "source": [
    "Logistic regression is a fundamental machine learning model for binary classification, predicting the probability of an input belonging to one of two classes (e.g., 0 or 1). The `MiniBatchLogisticRegression` class implements this using mini-batch gradient descent, sigmoid activation, and Elastic Net regularization. \n",
    "\n",
    "**Core Idea: Modeling Binary Probabilities**\n",
    "\n",
    "Logistic regression predicts the probability of an input $\\mathbf{x}$ belonging to the positive class (class 1). It computes a linear combination, $\\mathbf{\\theta}^T\\mathbf{x}$, where $\\mathbf{x}$ includes a bias term and $\\mathbf{\\theta}$ is the parameter vector. The **sigmoid function** converts this into a probability:\n",
    "\n",
    "$$\n",
    "P(y=1|\\mathbf{x}) = \\sigma(\\mathbf{\\theta}^T\\mathbf{x}) = \\frac{1}{1 + e^{-\\mathbf{\\theta}^T\\mathbf{x}}}\n",
    "$$\n",
    "\n",
    "The sigmoid’s S-shape ensures probabilities are between 0 and 1 and provides a smooth gradient for optimization. The `__sigmoid` method implements this, used in `fit` for training and `predict_proba` for probability outputs.\n",
    "\n",
    "**Loss Function: Log Loss**\n",
    "\n",
    "To optimize $\\mathbf{\\theta}$, the model minimizes the **log loss** (negative log-likelihood), which measures how well predicted probabilities match true labels. For $m$ samples, the loss is:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{\\theta}) = -\\frac{1}{m} \\sum_{i=1}^m \\left[ y^{(i)} \\log(\\sigma(\\mathbf{\\theta}^T \\mathbf{x}^{(i)})) + (1 - y^{(i)}) \\log(1 - \\sigma(\\mathbf{\\theta}^T \\mathbf{x}^{(i)})) \\right]\n",
    "$$\n",
    "\n",
    "This penalizes confident incorrect predictions. The `fit` method computes the gradient of this loss to update $\\mathbf{\\theta}$.\n",
    "\n",
    "**Gradient Computation and Mini-Batch Optimization**\n",
    "\n",
    "The gradient of the log loss drives parameter updates:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} J(\\theta) = \\frac{1}{m} X^T \\left( \\sigma(X\\theta) - y \\right) + \\lambda \\left( \\alpha \\cdot \\text{sign}(\\theta) + 2(1 - \\alpha) \\cdot \\theta \\right)\n",
    "$$\n",
    "\n",
    "This gradient depends on the prediction error $\\sigma(X\\theta) - y$ and includes regularization terms. The class uses **mini-batch gradient descent**, processing subsets (`batch_size=64`), shuffling data each epoch (`rnd_idx`). The update rule is:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\cdot \\nabla_{\\theta} J(\\theta)\n",
    "$$\n",
    "\n",
    "A **learning schedule** reduces the learning rate $\\eta$:\n",
    "\n",
    "$$\n",
    "\\eta(t) = \\frac{t_0}{t + t_1}\n",
    "$$\n",
    "\n",
    "Implemented in `learning_schedule`, this stabilizes convergence using `learning_schedule_t0` and `learning_schedule_t1`.\n",
    "\n",
    "**Regularization: Controlling Model Complexity**\n",
    "\n",
    "**Elastic Net regularization** prevents overfitting by adding a penalty to the loss:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m \\left[ y^{(i)} \\log(\\sigma(\\mathbf{\\theta}^T \\mathbf{x}^{(i)})) + (1 - y^{(i)}) \\log(1 - \\sigma(\\mathbf{\\theta}^T \\mathbf{x}^{(i)})) \\right] + \\lambda \\left( \\alpha |\\theta| + (1 - \\alpha) \\theta^2 \\right)\n",
    "$$\n",
    "\n",
    "- **L1 (Lasso)**: Adds $|\\theta|$, promoting sparsity.\n",
    "- **L2 (Ridge)**: Adds $\\theta^2$, encouraging small weights.\n",
    "\n",
    "Parameters $\\lambda$ (`lambda_`) and $\\alpha$ (`alpha`) control penalty strength and L1/L2 balance. The `fit` method includes `l1_grad` and `l2_grad` for generalization.\n",
    "\n",
    "**Decision Boundary and Predictions**\n",
    "\n",
    "Predictions use a threshold (`decision_threshould=0.5`):\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\begin{cases} \n",
    "1 & \\text{if } \\sigma(\\mathbf{\\theta}^T\\mathbf{x}) \\geq \\text{threshold} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The `predict` method computes $\\sigma(X\\theta)$ (`estimated_proba`) and applies the threshold, while `predict_proba` returns probabilities. The decision boundary is at $\\mathbf{\\theta}^T\\mathbf{x} = 0$ for a 0.5 threshold.\n",
    "\n",
    "This theory underpins the `MiniBatchLogisticRegression` class, enabling robust binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a8058090-eece-4783-8fc2-133c3c5b632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    Logistic Regression with Mini-batch Gradient Descent and Elastic Net Regularization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    eta : float, default=0.01\n",
    "        Learning rate. Must be > 0. Large values risk divergence, small values slow convergence.\n",
    "\n",
    "    n_epochs : int, default=1000\n",
    "        Number of epochs. Must be >= 1. Higher values improve convergence but increase training time.\n",
    "    \n",
    "    batch_size : int, default=64\n",
    "        Mini-batch size. Must be >= 1 and <= dataset size. Smaller batches add noise, larger ones increase computation.\n",
    "    \n",
    "    decision_threshould : float, default=0.5\n",
    "        Threshold for classifying probabilities into class labels (0 or 1). Must be in [0, 1].\n",
    "    \n",
    "    learning_schedule : bool, default=True\n",
    "        If True, decreases learning rate over iterations.\n",
    "    \n",
    "    learning_schedule_t0 : float, default=5\n",
    "        Controls initial learning rate decay. Must be > 0.\n",
    "    \n",
    "    learning_schedule_t1 : float, default=50\n",
    "        Controls decay rate. Must be > 0.\n",
    "    \n",
    "    alpha : float, default=0\n",
    "        Elastic Net mixing: 0 (L2/Ridge), 1 (L1/Lasso), (0,1) mix. Must be in [0,1].\n",
    "    \n",
    "    lambda_ : float, default=0.01\n",
    "        Regularization strength. Must be >= 0. Higher values increase regularization.\n",
    "    \n",
    "    random_state : int, default=42\n",
    "        Seed for reproducibility.\n",
    "    \"\"\"\n",
    "    def __init__(self,eta=0.01,n_epochs=1000,batch_size=64,decision_threshould=0.5,learning_schedule=True, \n",
    "                 learning_schedule_t0=5,learning_schedule_t1=50,alpha=0,lambda_=0.01, random_state=42):\n",
    "        self.eta = eta\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.decision_threshould = decision_threshould\n",
    "        self.learning_schedule = learning_schedule\n",
    "        self.learning_schedule_t0 = learning_schedule_t0\n",
    "        self.learning_schedule_t1 = learning_schedule_t1\n",
    "        self.alpha = alpha\n",
    "        self.lambda_ = lambda_\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def __sigmoid(self,z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        X = add_bias(X)\n",
    "        y = y.reshape(-1,1)\n",
    "        \n",
    "        np.random.seed(self.random_state)\n",
    "        self.theta = np.random.rand(X.shape[1],1)\n",
    "\n",
    "        def learning_schedule(t): # decreasing learning rate over iterations\n",
    "            return self.learning_schedule_t0 / (t + self.learning_schedule_t1)\n",
    "\n",
    "\n",
    "        update_count = 0\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            rnd_idx = np.random.permutation(len(X)) # Shuffling indexes\n",
    "            \n",
    "            for mini_batch_start in range(0, len(X) , self.batch_size):\n",
    "                X_batch = X[rnd_idx[mini_batch_start:mini_batch_start+self.batch_size]]\n",
    "                y_batch = y[rnd_idx[mini_batch_start:mini_batch_start+self.batch_size]]\n",
    "\n",
    "                l1_grad = self.lambda_ * self.alpha * np.sign(self.theta)\n",
    "                l2_grad = self.lambda_ * (1-self.alpha) * 2 * self.theta\n",
    "                \n",
    "                gradients = (1 / X_batch.shape[0]) * X_batch.T @ (self.__sigmoid(X_batch @ self.theta) - y_batch) + l1_grad + l2_grad \n",
    "                \n",
    "                if self.learning_schedule:\n",
    "                    self.eta = learning_schedule(update_count)\n",
    "                    \n",
    "                self.theta = self.theta - self.eta * gradients\n",
    "                update_count += 1\n",
    "                \n",
    "        return self\n",
    "\n",
    "    def predict(self,X):\n",
    "        X = add_bias(X)\n",
    "        self.estimated_proba = self.__sigmoid( X @ self.theta )\n",
    "        yhat = (self.estimated_proba >= self.decision_threshould).astype(int)\n",
    "        return yhat.reshape(-1)\n",
    "\n",
    "    def predict_proba(self):\n",
    "        return self.estimated_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "724ee0d4-5fe7-49c8-82dd-1257e2c1a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prepartion for binary classificaion\n",
    "\n",
    "# Generating data\n",
    "X , y = generate_poly_classification_data(n_samples=3000, n_features=10,n_classes=2, noise_std=1.0, random_state=42)\n",
    "\n",
    "# train/val split\n",
    "X_train , y_train  , X_val , y_val = train_test_split( X , y , random_state=42, train_size=0.8)\n",
    "\n",
    "# scaling will help faster convergence\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92a68d69-c97e-44f8-8329-b7c48cb692b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Accuracy:  0.9583\n",
      "Precision: 0.9573\n",
      "Recall:    0.9588\n",
      "F1 Score:  0.9580\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Testing & Evaluating the model class\n",
    "\n",
    "minbatchLogReg = MiniBatchLogisticRegression(eta=0.01,n_epochs=4000,batch_size=64,decision_threshould=0.5,learning_schedule=True, \n",
    "                                 learning_schedule_t0=5,learning_schedule_t1=50,alpha=0,lambda_=0.01, random_state=42)\n",
    "\n",
    "evaluate_classification(minbatchLogReg, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2fa01c-5fde-4e9d-9772-153ef75fe36f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.2. Softmax Regression (with Regularization & Learning Schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20d8bac-0f5a-4e0b-8d16-9bd3c3885a6c",
   "metadata": {},
   "source": [
    "Softmax regression, also known as multinomial logistic растворregression, extends logistic regression to handle multi-class classification, predicting the probability of an input belonging to one of $k$ classes. The `MiniBatchSoftmaxRegression` class implements this using mini-batch gradient descent, softmax activation, and Elastic Net regularization. Below, we explain the key concepts and formulations, focusing on a machine learning perspective with minimal math, tailored to clarify the provided implementation.\n",
    "\n",
    "**Core Idea: Modeling Multi-Class Probabilities**\n",
    "\n",
    "Softmax regression predicts the probability of an input $\\mathbf{x}$ belonging to each of $k$ classes. It computes a linear combination for each class, $Z = X\\Theta$, where $X$ is the input data (with a bias term) and $\\Theta$ is the parameter matrix (weights for each class). The **softmax function** converts these scores into probabilities:\n",
    "\n",
    "$$\n",
    "P(y=j|\\mathbf{x}) = \\text{softmax}(X\\Theta)_j = \\frac{\\exp((X\\Theta)_j)}{\\sum_{l=1}^k \\exp((X\\Theta)_l)}\n",
    "$$\n",
    "\n",
    "This ensures probabilities sum to 1. The `__softmax` method subtracts the maximum score per sample for numerical stability. The `predict_proba` method outputs these probabilities, while `predict` selects the class with the highest probability using `np.argmax`.\n",
    "\n",
    "**Loss Function: Cross-Entropy Loss**\n",
    "\n",
    "To optimize $\\Theta$, the model minimizes the **cross-entropy loss**, which measures how well predicted probabilities match true labels. For $m$ samples and $k$ classes, with one-hot encoded labels $Y$, the loss is:\n",
    "\n",
    "$$\n",
    "J(\\Theta) = -\\frac{1}{m} \\sum_{i=1}^m \\sum_{j=1}^k y_{ij} \\log(P(y=j|\\mathbf{x}^{(i)}))\n",
    "$$\n",
    "\n",
    "Here, $y_{ij} = 1$ if sample $i$ belongs to class $j$, else 0. The `fit` method uses one-hot encoded labels (`y_onehot`) to compute the loss’s gradient.\n",
    "\n",
    "**Gradient Computation and Mini-Batch Optimization**\n",
    "\n",
    "The gradient of the cross-entropy loss updates $\\Theta$:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\Theta} J(\\Theta) = \\frac{1}{m} X^T (P - Y) + \\lambda \\left( \\alpha \\cdot \\text{sign}(\\Theta) + 2(1 - \\alpha) \\cdot \\Theta \\right)\n",
    "$$\n",
    "\n",
    "Here, $P = \\text{softmax}(X\\Theta)$ is the predicted probability matrix, and $Y$ is the one-hot label matrix. Regularization terms prevent overfitting. The `fit` method computes this gradient for mini-batches (`batch_size=64`), shuffling data each epoch (`rnd_idx`). The update rule is:\n",
    "\n",
    "$$\n",
    "\\Theta = \\Theta - \\eta \\cdot \\nabla_{\\Theta} J(\\Theta)\n",
    "$$\n",
    "\n",
    "A **learning schedule** reduces the learning rate $\\eta$:\n",
    "\n",
    "$$\n",
    "\\eta(t) = \\frac{t_0}{t + t_1}\n",
    "$$\n",
    "\n",
    "Implemented in `learning_schedule`, this ensures stable convergence using `learning_schedule_t0` and `learning_schedule_t1`.\n",
    "\n",
    "**Regularization: Controlling Model Complexity**\n",
    "\n",
    "**Elastic Net regularization** prevents overfitting by adding a penalty to the loss:\n",
    "\n",
    "$$\n",
    "J(\\Theta) = -\\frac{1}{m} \\sum_{i=1}^m \\sum_{j=1}^k y_{ij} \\log(P(y=j|\\mathbf{x}^{(i)})) + \\lambda \\left( \\alpha |\\Theta| + (1-\\alpha) \\Theta^2 \\right)\n",
    "$$\n",
    "\n",
    "- **L1 (Lasso)**: Adds $|\\Theta|$, promoting sparsity.\n",
    "- **L2 (Ridge)**: Adds $\\Theta^2$, encouraging small weights.\n",
    "\n",
    "Parameters $\\lambda$ (`lambda_`) and $\\alpha$ (`alpha`) control penalty strength and L1/L2 balance. The `fit` method includes `l1_grad` and `l2_grad` for generalization.\n",
    "\n",
    "**Decision Boundary and Predictions**\n",
    "\n",
    "The `predict` method computes $X\\Theta$ (`logits`), applies softmax, and selects the class with the highest probability. No explicit threshold is needed, unlike logistic regression. The `predict_proba` method returns the probability distribution over classes.\n",
    "\n",
    "This theory underpins the `MiniBatchSoftmaxRegression` class, enabling robust multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c6373604-8965-47e9-b64c-f3a77ed933a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression:\n",
    "    \"\"\"\n",
    "    Softmax Regression with Mini-batch Gradient Descent and Elastic Net Regularization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    eta : float, default=0.01\n",
    "        Learning rate. Must be > 0. Large values risk divergence, small values slow convergence.\n",
    "    \n",
    "    n_epochs : int, default=1000\n",
    "        Number of epochs. Must be >= 1. Higher values improve convergence but increase training time.\n",
    "    \n",
    "    batch_size : int, default=64\n",
    "        Mini-batch size. Must be >= 1 and <= dataset size. Smaller batches add noise, larger ones increase computation.\n",
    "    \n",
    "    learning_schedule : bool, default=True\n",
    "        If True, decreases learning rate over iterations.\n",
    "    \n",
    "    learning_schedule_t0 : float, default=5\n",
    "        Controls initial learning rate decay. Must be > 0.\n",
    "    \n",
    "    learning_schedule_t1 : float, default=50\n",
    "        Controls decay rate. Must be > 0.\n",
    "    \n",
    "    alpha : float, default=0\n",
    "        Elastic Net mixing: 0 (L2/Ridge), 1 (L1/Lasso), (0,1) mix. Must be in [0,1].\n",
    "    \n",
    "    lambda_ : float, default=0.01\n",
    "        Regularization strength. Must be >= 0. Higher values increase regularization.\n",
    "    \n",
    "    random_state : int, default=42\n",
    "        Seed for reproducibility.\n",
    "    \"\"\"\n",
    "    def __init__(self,eta=0.01,n_epochs=1000,batch_size=64,learning_schedule=True, learning_schedule_t0=5,\n",
    "                 learning_schedule_t1=50,alpha=0,lambda_=0.01, random_state=42):\n",
    "        self.eta = eta\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_schedule = learning_schedule\n",
    "        self.learning_schedule_t0 = learning_schedule_t0\n",
    "        self.learning_schedule_t1 = learning_schedule_t1\n",
    "        self.alpha = alpha\n",
    "        self.lambda_ = lambda_\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def __softmax(self,Z):\n",
    "        Z_stable = Z - np.max(Z, axis=1, keepdims=True) # subtracting max of each row for numerical stability\n",
    "        exp_Z = np.exp(Z_stable)\n",
    "        return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        X = add_bias(X)\n",
    "        y_onehot = OneHotEncoding(y)\n",
    "        \n",
    "        np.random.seed(self.random_state)\n",
    "        self.Theta = np.random.rand(X.shape[1],y_onehot.shape[1])\n",
    "        \n",
    "        def learning_schedule(t): # decreasing learning rate over iterations\n",
    "            return self.learning_schedule_t0 / (t + self.learning_schedule_t1)\n",
    "\n",
    "        update_count = 0\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            rnd_idx = np.random.permutation(len(X)) # Shuffling indexes\n",
    "            \n",
    "            for mini_batch_start in range(0, len(X) , self.batch_size):\n",
    "                X_batch = X[rnd_idx[mini_batch_start:mini_batch_start+self.batch_size]]\n",
    "                y_batch = y_onehot[rnd_idx[mini_batch_start:mini_batch_start+self.batch_size]]\n",
    "\n",
    "                l1_grad = self.lambda_ * self.alpha * np.sign(self.Theta)\n",
    "                l2_grad = self.lambda_ * (1-self.alpha) * 2 * self.Theta\n",
    "\n",
    "                gradients = (1 / X_batch.shape[0]) * X_batch.T @ (self.__softmax(X_batch @ self.Theta) - y_batch) + l1_grad + l2_grad \n",
    "                \n",
    "                \n",
    "                if self.learning_schedule:\n",
    "                    self.eta = learning_schedule(update_count)\n",
    "                    \n",
    "                self.Theta = self.Theta - self.eta * gradients\n",
    "                update_count += 1\n",
    "                \n",
    "        return self\n",
    "\n",
    "    def predict(self,X):\n",
    "        X = add_bias(X)\n",
    "        self.logits = X @ self.Theta\n",
    "        yhat = np.argmax(self.logits,  axis=1)\n",
    "        return yhat.reshape(-1,1)\n",
    "\n",
    "    def predict_proba(self):\n",
    "        return self.__softmax(self.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b3d73923-addb-4749-bd49-4e92f00f4e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prepartion for multi class classificaion\n",
    "\n",
    "# Generating data\n",
    "X , y = generate_poly_classification_data(n_samples=3000, n_features=10 , n_classes=4 , noise_std=1.0 , random_state=42)\n",
    "\n",
    "# train/val split\n",
    "X_train , y_train  , X_val , y_val = train_test_split( X , y , random_state=42, train_size=0.8)\n",
    "\n",
    "# scaling will help faster convergence\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8e0417a3-05e6-4c1b-96fd-ff01caf97b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Accuracy:  0.9583\n",
      "Precision: 0.9723\n",
      "Recall:    0.6993\n",
      "F1 Score:  0.8135\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Testing & Evaluating the model class\n",
    "\n",
    "minbatchSoftMaxRegression = MiniBatchSoftmaxRegression(eta=0.01,n_epochs=4000,batch_size=64,learning_schedule=True, learning_schedule_t0=5,\n",
    "                                                     learning_schedule_t1=50,alpha=0,lambda_=0.01, random_state=42)\n",
    "\n",
    "evaluate_classification(minbatchSoftMaxRegression, X_train, y_train, X_val, y_val, average=\"macro\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
